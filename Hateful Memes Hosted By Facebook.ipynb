{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::opts_chunk$set(echo = FALSE)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(jsonlite)\n",
                "library(tidyverse)\n",
                "library(ggplot2)\n",
                "library(caret)\n",
                "library(quanteda)\n",
                "library(tidytext)\n",
                "library(tm)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df<- readLines(\"train.jsonl\") %>% \n",
                "  lapply(fromJSON) %>% \n",
                "  lapply(unlist) %>% \n",
                "  bind_rows()\n",
                "\n",
                "train_df$id<- as.numeric(train_df$id)\n",
                "\n",
                "library(lfactors)\n",
                "train_df$label_name<- lfactor(train_df$label, levels = 0:1, \n",
                "                         labels =  c(\"non-hateful\", \"hateful\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dev_df<- readLines(\"dev.jsonl\") %>% \n",
                "  lapply(fromJSON) %>% \n",
                "  lapply(unlist) %>% \n",
                "  bind_rows() %>% \n",
                "  select(- img)\n",
                "\n",
                "dev_df$id<- as.numeric(dev_df$id)\n",
                "\n",
                "\n",
                "dev_df$label_name<- lfactor(dev_df$label, levels = 0:1, \n",
                "                         labels =  c(\"non-hateful\", \"hateful\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df %>%\n",
                "  ggplot(aes(label_name))+\n",
                "  geom_bar()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df %>%\n",
                "   mutate(text = removeWords(text, stopwords(\"english\")),\n",
                "          text = removeNumbers(text)) %>% \n",
                "   unnest_ngrams(bigram, text, n=3) %>%\n",
                "   filter(!str_detect(bigram, \"^[0-9]*$\")) %>%\n",
                "#   anti_join(stop_words) %>%\n",
                " #  mutate(word = SnowballC::wordStem(word)) %>% \n",
                "   group_by(label_name) %>% \n",
                "   count(bigram, sort = T) %>%\n",
                "   top_n(15, wt = n) %>% \n",
                "   ungroup() %>% \n",
                "   mutate(bigram = reorder_within(bigram, n, label_name)) %>% \n",
                "   ggplot(aes(bigram, n))+\n",
                "   geom_col()+\n",
                "   scale_x_reordered()+\n",
                "   xlab(NULL)+\n",
                "   facet_wrap(~label_name, scales = \"free\")+\n",
                "   coord_flip()\n",
                "\n",
                "train_df %>%\n",
                "   select(-img) %>%\n",
                "   unnest_tokens(word, text) %>%\n",
                "   mutate(word = SnowballC::wordStem(word),\n",
                "          word = ifelse(grepl(\"ω\", word), \"omega\", word),\n",
                "          word = ifelse(grepl(\"america\", word), \"america\", word),\n",
                "          word = ifelse(grepl(\"allah\", word), \"allah\", word),\n",
                "          word = removeNumbers(word),\n",
                "          word = removeWords(word, stopwords(\"english\")),\n",
                "          word = gsub(\"[[:punct:]]\", \" \", word)) %>% \n",
                "  count(label_name, word) %>% \n",
                "  bind_tf_idf(word,label_name, n) %>% \n",
                "  group_by(label_name) %>%\n",
                "  top_n(10) %>%\n",
                "  ungroup() %>%\n",
                "  mutate(word = reorder_within(word, tf_idf, label_name)) %>%\n",
                "  ggplot(aes(word, tf_idf)) +\n",
                "  geom_col() +\n",
                "  scale_x_reordered() +\n",
                "  labs(x = NULL, y = \"tf-idf\") +\n",
                "  facet_wrap(~ label_name, scales = \"free\") +\n",
                "  coord_flip()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_token<- train_df %>%\n",
                "   select(-img) %>%\n",
                "   mutate(text = removeWords(text, stopwords(\"english\"))) %>%   \n",
                "   unnest_tokens(word, text) %>%\n",
                "   mutate(word = SnowballC::wordStem(word),\n",
                "          word = ifelse(grepl(\"ω\", word), \"omega\", word),\n",
                "          word = ifelse(grepl(\"america\", word), \"america\", word),\n",
                "          word = ifelse(grepl(\"allah\", word), \"allah\", word),\n",
                "          word = removeNumbers(word),\n",
                "          word = stripWhitespace(word),\n",
                "          word = gsub(\"[[:punct:]]\", \" \", word)) %>% \n",
                "    mutate_if(is.character, list(~na_if(.,\" \"))) %>% \n",
                "    na.omit() %>% \n",
                "    group_by(label,id) %>% \n",
                "    summarize(text = str_c(word, collapse = \" \")) %>%\n",
                "    ungroup() %>% \n",
                "   arrange(id)\n",
                "\n",
                "train_dtm <- train_df %>%\n",
                "   select(-img) %>%\n",
                "   mutate(text = removeWords(text, stopwords(\"english\"))) %>%   \n",
                "   unnest_tokens(word, text) %>%\n",
                "   mutate(word = SnowballC::wordStem(word),\n",
                "          word = ifelse(grepl(\"ω\", word), \"omega\", word),\n",
                "          word = ifelse(grepl(\"america\", word), \"america\", word),\n",
                "          word = ifelse(grepl(\"allah\", word), \"allah\", word),\n",
                "          word = removeNumbers(word),\n",
                "          word = stripWhitespace(word),\n",
                "          word = gsub(\"[[:punct:]]\", \" \", word)) %>% \n",
                "    mutate_if(is.character, list(~na_if(.,\" \"))) %>% \n",
                "    na.omit() %>%\n",
                "    count(id, word) %>% \n",
                "    cast_dtm(id, word, n)\n",
                "\n",
                "\n",
                "train_dtm_2<-removeSparseTerms(train_dtm, sparse = .99)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(wordcloud)\n",
                "set.seed(1234)\n",
                "\n",
                "words<- train_df%>%\n",
                "  unnest_tokens(word, text) %>% \n",
                "  mutate(word = ifelse(grepl(\"ω\", word), \"omega\", word),\n",
                "         word = SnowballC::wordStem(word),\n",
                "         word = removeNumbers(word),\n",
                "         word = removeWords(word, stopwords(\"english\")),\n",
                "         word = gsub(\"[[:punct:]]\", \" \", word),\n",
                "         word = ifelse(str_length(word)<=2, \" \", word)) %>%\n",
                "  count(word, sort =T)\n",
                "\n",
                "\n",
                "\n",
                "wordcloud(words = words$word, freq = words$n,min.freq = 15,\n",
                "          max.words=500,  rot.per=0.35, \n",
                "          colors=brewer.pal(8, \"Dark2\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_corpus <- VCorpus(VectorSource(train_df$text))\n",
                "\n",
                "toSpace<- content_transformer(function(x,pattern)\n",
                "  { return(gsub(pattern, \" \", x))})\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "tc<- train_corpus %>% \n",
                "  tm_map(removePunctuation) %>% \n",
                "  tm_map(removeNumbers) %>% \n",
                "  tm_map(tolower) %>%\n",
                "  tm_map(gsub, pattern = \"america\", replacement = \"america\") %>% \n",
                "  tm_map(gsub, pattern = \"allah\", replacement = \"allah\") %>% \n",
                "  tm_map(removeWords, stopwords(\"english\")) %>% \n",
                "  tm_map(stemDocument) %>% \n",
                "  tm_map(stripWhitespace) %>%\n",
                "  tm_map(PlainTextDocument) %>% \n",
                "  DocumentTermMatrix()\n",
                "\n",
                "  \n",
                "  \n",
                "  \n",
                "  for (i in 1:100) {\n",
                "    cat(paste(\"[[\", i, \"]] \", sep = \"\"))\n",
                "    #writeLines(myCorpus[[i]])\n",
                "    writeLines(as.character(tc[[i]]))\n",
                "}\n",
                "\n",
                "  \n",
                "tc_df<- tidy(tc)\n",
                "\n",
                "tc_df %>% \n",
                "  filter(grepl(\"america|allah\", term)) %>% \n",
                "  count(term, sort = T)\n",
                "\n",
                "tc_df %>% \n",
                "  arrange(desc(count))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(caret)\n",
                "library(tictoc)\n",
                "tic()\n",
                "train_rf <- train(x = as.matrix(train_dtm),\n",
                "                     y = train_token$label,\n",
                "                     method = \"ranger\",\n",
                "                     num.trees = 80,\n",
                "                     trControl = trainControl(method = \"oob\"))\n",
                "toc()\n",
                "\n",
                "\n",
                "summary(train_rf)\n",
                "\n",
                "train_rf$predictions\n",
                "\n",
                "train_glm2 <- train(x = x[,-4],\n",
                "                   y = train_token$label,\n",
                "  trControl = trainControl(method = \"cv\", number = 5),\n",
                "  method = \"glm\",\n",
                "  family = \"binomial\")\n",
                "\n",
                "train_glm <- train(x = as.matrix(train_dtm),\n",
                "                   y = train_token$label,\n",
                "  trControl = trainControl(method = \"cv\", number = 5),\n",
                "  method = \"glm\",\n",
                "  family = \"binomial\")\n",
                "\n",
                "summary(train_glm2)\n",
                "\n",
                "library(modelr)\n",
                "train_token %>% \n",
                "  add_predictions(train_glm2)\n",
                "\n",
                "\n",
                "train_token %>% \n",
                "  filter( id %in% c(1235:1274)) %>% \n",
                "  select(id, label) %>% \n",
                "  arrange(id)\n",
                "\n",
                "p.data<- predict(train_glm2, type = \"prob\")\n",
                "\n",
                "p.data %>% \n",
                "  mutate(id = train_token$id) %>% \n",
                "  pivot_longer(`non-hateful`:hateful, names_to = \"label\", values_to = \"response\") %>% \n",
                "  mutate( response = ifelse(response>0.5, 1, 0)) %>% \n",
                "  pivot_wider(names_from= \"label\", values_from = \"response\") %>% \n",
                "  count(`non-hateful`,hateful)\n",
                "  \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(textrecipes)\n",
                "library(tidymodels)\n",
                "train_df1<-    train_df %>%\n",
                "   select(-img) %>%\n",
                "   group_by(label) %>% \n",
                "   mutate(line_num = row_number()) %>% \n",
                "   ungroup() %>% \n",
                "   unnest_tokens(word, text) %>%\n",
                "   mutate(word = SnowballC::wordStem(word),\n",
                "          word = ifelse(grepl(\"ω\", word), \"omega\", word),\n",
                "          word = ifelse(grepl(\"america\", word), \"america\", word),\n",
                "          word = ifelse(grepl(\"allah\", word), \"allah\", word),\n",
                "          word = removeNumbers(word),\n",
                "          word = gsub(\"[[:punct:]]\", \" \", word)) %>% \n",
                "    mutate_if(is.character, list(~na_if(.,\" \"))) %>% \n",
                "    na.omit() %>% \n",
                "   group_by(label,id) %>% \n",
                "   summarize(text = str_c(word, collapse = \" \")) %>%\n",
                "   ungroup() %>% \n",
                "   mutate(text = gsub(\"[[:punct:]]\", \"\", text))\n",
                "\n",
                "train_rec <- recipe(label ~ text, data = train_df1) %>%\n",
                "  step_tokenize(text) %>%\n",
                "  step_stopwords(text) %>%\n",
                "  step_tokenfilter(text) %>%\n",
                "  step_tfidf(text) %>%\n",
                "  step_normalize(all_predictors())\n",
                "\n",
                "train_prep<- prep(train_rec)\n",
                "\n",
                "train_prep\n",
                "\n",
                "\n",
                "lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%\n",
                "  set_engine(\"glmnet\")\n",
                "\n",
                "lasso_wf <- workflow() %>%\n",
                "  add_recipe(train_rec) %>%\n",
                "  add_model(lasso_spec)\n",
                "\n",
                "lasso_wf\n",
                "\n",
                "lambda_grid <- grid_regular(penalty(), levels = 3)\n",
                "set.seed(123)\n",
                "review_folds <- bootstraps(train_df1, strata = label)\n",
                "review_folds\n",
                "\n",
                "\n",
                "doParallel::registerDoParallel()\n",
                "\n",
                "set.seed(2020)\n",
                "lasso_grid <- tune_grid(\n",
                "  lasso_wf,\n",
                "  resamples = review_folds,\n",
                "  grid = lambda_grid,\n",
                "  metrics = metric_set(roc_auc))\n",
                "\n",
                "lasso_grid %>%\n",
                "  collect_metrics()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(h2o4gpu)\n",
                "library(reticulate)  # only needed if using a virtual Python environment\n",
                "use_virtualenv(\"/home/ledell/venv/h2o4gpu\")  # set this to the path of your venv\n",
                "\n",
                "train_token %>% \n",
                "  glimpse()\n",
                "\n",
                "\n",
                "train_x<- as.matrix(train_dtm)\n",
                "train_y<-  train_token %>% select(label)\n",
                "\n",
                "model_enc<- h2o4gpu.elastic_net_classifier() %>% \n",
                "                fit(train_x, train_y)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Using Udpipe model for lemmatisation\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(udpipe)\n",
                "# download current udpipe model for English\n",
                "udtarget <- udpipe_download_model(language = \"english\",\n",
                "                                  model_dir = tempdir())\n",
                "# load the model\n",
                "udmodel <- udpipe_load_model(file = udtarget$file_model) \n",
                "\n",
                "words <- udpipe_annotate(udmodel, x = train_df$text, doc_id = train_df$id) %>% \n",
                "  as.data.frame() %>%\n",
                "  select(id = doc_id, token, lemma, upos, sentence_id) %>%\n",
                "  mutate(id = as.numeric(id))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vocabulary <-  words %>%\n",
                "  count(lemma) %>%\n",
                "  ungroup() %>%\n",
                "  mutate(lemma = removeNumbers(lemma),\n",
                "          lemma = stripWhitespace(lemma),\n",
                "          lemma = removeWords(lemma, stopwords(\"english\")),\n",
                "          lemma = gsub(\"[[:punct:]]\", \"\", lemma)) %>% \n",
                "  arrange(desc(n)) %>%\n",
                "  filter(n >= 5 ) %>%\n",
                "  filter(str_detect(lemma, \"\")) %>% # little prediction value in rarer words\n",
                "  mutate(id_slovo = row_number()) %>% # unique id per lemma\n",
                "  select(lemma, id_slovo)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 150 zeroes for each tweet id for padding\n",
                "vata <- expand.grid(id = unique(words$id),\n",
                "                    word_num = 1:150,\n",
                "                    id_slovo = 0)\n",
                "\n",
                "word_matrix <-   words %>% # words\n",
                "  # filtering join! words not in vocabulary are discarded\n",
                "  inner_join(vocabulary, by = c('lemma' = 'lemma')) %>% \n",
                "  select(id, lemma, id_slovo) %>%\n",
                "  group_by(id) %>%\n",
                "  mutate(word_num = row_number()) %>% # \n",
                "  ungroup() %>%\n",
                "  select(id, word_num, id_slovo) %>% # relevant columns\n",
                "  rbind(vata) %>% # bind the 150 zeroes per meme\n",
                "  group_by(id, word_num) %>%\n",
                "  mutate(id_slovo = max(id_slovo)) %>% # will include duplicites\n",
                "  ungroup() %>%\n",
                "  unique() %>% # remove duplicites\n",
                "  spread(word_num, id_slovo) # spread to matrix format\n",
                "\n",
                "keras_input <- train_df %>%\n",
                "  select(id, label, text) %>%\n",
                "  inner_join(word_matrix, by = c('id' = 'id'))\n",
                "\n",
                "glimpse(keras_input)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "set.seed(42) # Zaphod Beeblebrox advises to trust no other!\n",
                "\n",
                "idx <- createDataPartition(keras_input$label, p = .8, list = F, times = 1) # 80 / 20 split\n",
                "\n",
                "train_data <- keras_input[idx, ] # train dataset\n",
                "test_data <- keras_input[-idx, ]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_data <- train_data %>%\n",
                "  mutate(hateful = ifelse(label == 'hateful', 1,0)) %>% # binary output\n",
                "  select(-id, -label, -text)\n",
                "\n",
                "x_train <- data.matrix(train_data %>% select(-hateful)) # everything except target\n",
                "y_train <- data.matrix(train_data %>% select(hateful)) # target, and target only\n",
                "\n",
                "vocab_size <- vocabulary %>% # count unique word ids\n",
                "  pull(id_slovo) %>% \n",
                "  unique() %>%\n",
                "  length() + 1 # one extra for the zero padding\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(keras)\n",
                "library(tensorflow)\n",
                "#tensorflow::use_session_with_seed(1234, disable_gpu = F, disable_parallel_cpu = F)\n",
                "model <- keras_model_sequential() %>% \n",
                "  layer_embedding(input_dim = vocab_size, output_dim = 256) %>%\n",
                "  bidirectional(layer_lstm(units = 128)) %>%\n",
                "  layer_dropout(rate = 0.5) %>% \n",
                "  layer_dense(units = 1, activation = 'sigmoid')  %>% \n",
                "  compile(optimizer = \"rmsprop\",\n",
                "          loss = \"binary_crossentropy\",\n",
                "          metrics = c(\"accuracy\"))\n",
                "\n",
                "history <- model %>%  # fit the model (this will take a while...)\n",
                "  fit(x_train, \n",
                "      y_train, \n",
                "      epochs = 20, \n",
                "      batch_size = 64, \n",
                "      validation_split = 0.2)\n",
                "\n",
                "summary(model)\n",
                "plot(history)\n",
                "\n",
                "\n",
                "#tensorflow::use_session_with_seed(124, disable_gpu = F, disable_parallel_cpu = F)\n",
                "#runs <- tuning_run(\"experiment.R\", flags = list(dense_units = c(32, 64)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model <- keras_model_sequential() \n",
                "\n",
                "model %>% \n",
                "  layer_dense(units = 64, \n",
                "              activation = 'sigmoid',\n",
                "              input_shape = c(150)) %>% \n",
                "  layer_dropout(rate = 0.1) %>% \n",
                "  layer_dense(units = 32,\n",
                "              activation = 'sigmoid') %>% \n",
                "  layer_dropout(rate = 0.1) %>% \n",
                "  layer_dense(units = 16,\n",
                "              activation = 'sigmoid') %>% \n",
                "  layer_dropout(rate = 0.1) %>% \n",
                "  layer_dense(units = 1, activation = 'sigmoid')\n",
                "# Compile\n",
                "model %>% compile(loss = 'binary_crossentropy',\n",
                "                  optimizer = 'rmsprop',\n",
                "                  metrics = 'accuracy')\n",
                "# Fit\n",
                "history <- model %>% \n",
                "  fit(x_train,\n",
                "      y_train,\n",
                "      epochs = 50,\n",
                "      batch_size = 32,\n",
                "      validation_split = 0.2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pred_data <- test_data %>% # expected results\n",
                "  select(id, label, text)\n",
                "\n",
                "test_data <- test_data %>%\n",
                "  mutate(hateful = ifelse(label == 'hateful', 1,0)) %>% # binary output\n",
                "  select(-id, -label, -text) # no cheating!\n",
                "\n",
                "x_pred <- data.matrix(test_data %>% select(-hateful)) # keras needs matrix\n",
                "\n",
                "pred <- model %>% # let keras sweat...\n",
                "  predict_proba(x_pred)\n",
                "\n",
                "verifikace <- pred_data %>% # correct results ...\n",
                "  cbind(pred) # ... joined with predictions\n",
                "\n",
                "verifikace <- verifikace %>%\n",
                "  mutate(label_pred = ifelse(pred > 0.5, 'hateful', 'non-hateful'))\n",
                "\n",
                "conf_mtx <- table(verifikace$label, verifikace$label_pred)\n",
                "\n",
                "print(paste0('Correctly predicted ',\n",
                "             sum(diag(conf_mtx)), ' of ',\n",
                "             sum(conf_mtx), ' meems, which means ', \n",
                "             round(100 * sum(diag(conf_mtx))/sum(conf_mtx), 2), \n",
                "             '% of the total.'))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
